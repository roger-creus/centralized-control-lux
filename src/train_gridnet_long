#!/bin/bash
#SBATCH --partition=long
#SBATCH --nodes=2
#SBATCH --gres=gpu:rtx8000:1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=2
#SBATCH --mem=10G
#SBATCH --time=2:00:00                                           
#SBATCH -o /network/scratch/r/roger.creus-castanyer/slurm-%j.out 

# 1. Load your environment
module load anaconda/3

### change 5-digit MASTER_PORT as you wish, slurm will raise Error if duplicated with others
export MASTER_PORT=12340

### change WORLD_SIZE as gpus/node * num_nodes
export WORLD_SIZE=2

echo "NODELIST="${SLURM_NODELIST}
master_addr=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
export MASTER_ADDR=$master_addr
echo "MASTER_ADDR="$MASTER_ADDR

conda activate /home/mila/r/roger.creus-castanyer/anaconda3/envs/mario

# 4. Launch your job, tell it to save the model in $SLURM_TMPDIR
#    and look for the dataset into $SLURM_TMPDIR

srun python3 ppo_final_gridnet_multigpu_postmasks.py --self-play True  --pool-size 5 --sparse-reward False --simple-obs False --track --num-envs 2 --num-steps 64 --eval-interval 50 --save-every 25

# 5. Copy whatever you want to save on $SCRATCH
cp -r $SLURM_TMPDIR/* /network/scratch/r/roger.creus-castanyer/