#!/bin/bash

#SBATCH --partition=lab-real                           # Ask for unkillable job
#SBATCH --cpus-per-task=12                               # Ask for 6 CPUs
#SBATCH --gres=gpu:4                                   # Ask for 8 GPU
#SBATCH --mem=500G                                        # Ask for 10 GB of RAM
#SBATCH --time=170:00:00                                   # The job will run for 3 hours
#SBATCH -o /network/scratch/r/roger.creus-castanyer/slurm-%j.out  # Write the log on scratch

# 1. Load your environment
module load anaconda/3

conda activate /home/mila/r/roger.creus-castanyer/anaconda3/envs/mario

# 4. Launch your job, tell it to save the model in $SLURM_TMPDIR
#    and look for the dataset into $SLURM_TMPDIR

torchrun --standalone --nnodes 1 --nproc_per_node 8 ppo_gridnet_multigpu.py --prod-mode --num-envs 512 --num-steps 128 --n-minibatch 4 --update-epochs 4 --seed 200 --device-ids 0 0 1 1 2 2 3 3 --save-every 30 --load-every 5 --save-path $SLURM_TMPDIR

# 5. Copy whatever you want to save on $SCRATCH
cp -r $SLURM_TMPDIR/* /network/scratch/r/roger.creus-castanyer/