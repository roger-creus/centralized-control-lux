#!/bin/bash

#SBATCH --partition=lab-real                           # Ask for unkillable job
#SBATCH --cpus-per-task=8                               # Ask for 6 CPUs
#SBATCH --gres=gpu:1                                   # Ask for 8 GPU
#SBATCH --mem=100G                                        # Ask for 10 GB of RAM
#SBATCH --time=170:00:00                                   # The job will run for 3 hours
#SBATCH -o /network/scratch/r/roger.creus-castanyer/slurm-%j.out  # Write the log on scratch

# 1. Load your environment
module load anaconda/3

conda activate /home/mila/r/roger.creus-castanyer/anaconda3/envs/mario

# 4. Launch your job, tell it to save the model in $SLURM_TMPDIR
#    and look for the dataset into $SLURM_TMPDIR

python3 ppo_gridnet.py --gamma 0.9995 --ent-coef 0.0001 --prod-mode --num-steps 256 --n-minibatch 4 --update-epochs 4 --seed 200 --save-every 15 --load-every 3 --save-path $SLURM_TMPDIR

# 5. Copy whatever you want to save on $SCRATCH
cp -r $SLURM_TMPDIR/* /network/scratch/r/roger.creus-castanyer/